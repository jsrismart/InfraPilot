# App Configuration
APP_NAME=InfraPilot
ALLOW_ORIGINS=["http://localhost:5173", "http://localhost:3000", "http://localhost:8080"]

# Ollama Configuration
OLLAMA_MODEL=mistral
# Alternative models (uncomment to use after installing with: ollama pull MODEL_NAME):
# OLLAMA_MODEL=neural-chat   # Balanced (~30-45 sec)
# OLLAMA_MODEL=phi           # Very fast but basic (~20-30 sec)
# OLLAMA_MODEL=qwen2.5-coder # Slow but accurate (~180+ sec)

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=300

# Performance Tuning
SKIP_TOOLS_BY_DEFAULT=False
# Set to True to skip Terraform/Checkov/Infracost by default (fast mode)
