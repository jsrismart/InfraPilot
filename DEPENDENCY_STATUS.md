# InfraPilot - Dependency Status Report

## âœ… All Required Dependencies Installed

### System Requirements
- **Python**: 3.14.0 âœ…
- **Node.js**: v24.11.1 âœ…
- **npm**: 11.6.2 âœ…

### Core Services
- **Ollama**: 0.13.0 âœ…
  - Status: **NOT RUNNING** âš ï¸
  - Action: Start with `ollama serve` in a separate terminal

### Backend Python Packages
| Package | Version | Status |
|---------|---------|--------|
| fastapi | Latest | âœ… Installed |
| uvicorn | Latest | âœ… Installed |
| pydantic | Latest | âœ… Installed |
| pydantic-settings | Latest | âœ… Installed |
| ollama | Latest | âœ… Installed |
| python-dotenv | 1.2.1 | âœ… Installed |

### Frontend Packages
- **node_modules**: âœ… Installed
- **vite**: Ready
- **react**: Ready
- **typescript**: Ready
- **tailwindcss**: Ready

### Optional Tools
| Tool | Status |
|------|--------|
| Terraform | âœ… v1.14.0 |
| Checkov | âš ï¸ Not installed (optional) |
| Infracost | âš ï¸ Not installed (optional) |

## ğŸš€ Quick Start

### Option 1: Automated Start (Recommended)
```powershell
cd infrapilot
.\start-all.ps1
```

### Option 2: Manual Start

**Terminal 1 - Start Ollama:**
```bash
ollama serve
```

**Terminal 2 - Start Backend:**
```bash
cd backend
python -m uvicorn app.main:app --host 0.0.0.0 --port 8001
```

**Terminal 3 - Start Frontend:**
```bash
cd frontend
npm run dev
```

Then open: **http://localhost:3001**

## ğŸ“‹ Verification Scripts

### Check Dependencies
```powershell
.\check-dependencies.ps1
```
This will:
- âœ… Verify all required modules are installed
- ğŸ“¦ Auto-install any missing packages
- ğŸ“Š Show a detailed status report

### Download Models
```powershell
.\setup-models.ps1
```
This will:
- ğŸ“¥ Download faster models (optional)
- âš™ï¸ Configure .env file
- ğŸ“ˆ Improve performance

## ğŸ”§ Configuration Files

### Backend Configuration
**File**: `backend/.env`
```env
OLLAMA_MODEL=qwen2.5-coder
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=300
```

### Frontend Configuration
**File**: `frontend/.env`
```env
VITE_API_BASE_URL=http://localhost:8001/api/v1
```

## ğŸ“Š Service URLs

| Service | URL | Port |
|---------|-----|------|
| Frontend | http://localhost:3001 | 3001 |
| Backend API | http://localhost:8001 | 8001 |
| Ollama API | http://localhost:11434 | 11434 |

## ğŸ› Troubleshooting

### Issue: "Ollama is not running"
**Solution:**
```bash
ollama serve
```

### Issue: "Port already in use"
**Solution:** Kill existing process:
```powershell
# For port 8001 (backend)
Get-NetTcpConnection -LocalPort 8001 -ErrorAction SilentlyContinue | Stop-Process -Force

# For port 3001 (frontend)
Get-NetTcpConnection -LocalPort 3001 -ErrorAction SilentlyContinue | Stop-Process -Force
```

### Issue: "Module not found"
**Solution:** Run dependency checker:
```powershell
.\check-dependencies.ps1
```

### Issue: "Model not found"
**Solution:** Download model:
```bash
ollama pull qwen2.5-coder
# or
ollama pull neural-chat
```

## ğŸ“ˆ Performance Tips

1. **Use Fast Mode** - Toggle in UI for 30-45 second responses
2. **Install faster model**:
   ```powershell
   .\setup-models.ps1
   # Select option 2 (neural-chat)
   ```
3. **Check Ollama is running** - `ollama serve`
4. **Free up RAM** - Close unnecessary applications

## âœ¨ Features Ready

- âœ… Infrastructure as Code generation (IaC)
- âœ… Terraform validation & planning
- âœ… Security scanning (with Checkov)
- âœ… Cost analysis (with Infracost)
- âœ… Parallel agent execution
- âœ… Fast Mode for quick generation
- âœ… Configurable models
- âœ… Error handling & logging

## ğŸ“š Documentation

- `PERFORMANCE_GUIDE.md` - Detailed performance tuning
- `OPTIMIZATION_SUMMARY.md` - Quick optimization reference
- `check-dependencies.ps1` - Verify all modules
- `setup-models.ps1` - Download Ollama models
- `start-all.ps1` - Quick service startup

---

**Status**: âœ… Ready to use - Just start Ollama and run services!
